{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15/11/2024\n",
    "\n",
    "# !pip install huggingface huggingface_hub\n",
    "# from huggingface_hub import HfApi\n",
    "# from transformers import Wav2Vec2Config, Wav2Vec2Model\n",
    "\n",
    "!export HF_DATASETS_CACHE=\"/home2/havt/emotion_intensity/cache\"\n",
    "CACHE_DIR = \"/home2/havt/emotion_intensity/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor, Wav2Vec2ForAudioFrameClassification, Wav2Vec2PreTrainedModel, Wav2Vec2Model, Wav2Vec2Config\n",
    "import torch\n",
    "from torch import nn\n",
    "# load model and tokenizer\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\", cache_dir=CACHE_DIR)\n",
    "# model = Wav2Vec2ForAudioFrameClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "_HIDDEN_STATES_START_POSITION = 2\n",
    "\n",
    "class Wav2Vec2ForEmotionIntensityAndClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        if hasattr(config, \"add_adapter\") and config.add_adapter:\n",
    "            raise ValueError(\n",
    "                \"Audio frame classification does not support the use of Wav2Vec2 adapters (config.add_adapter=True)\"\n",
    "            )\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        num_layers = config.num_hidden_layers + 1  # transformer layers + input embeddings\n",
    "        if config.use_weighted_layer_sum:\n",
    "            self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        \"\"\"\n",
    "        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n",
    "        not be updated during training.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n",
    "            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "        self.freeze_feature_encoder()\n",
    "\n",
    "    def freeze_feature_encoder(self):\n",
    "        \"\"\"\n",
    "        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n",
    "        not be updated during training.\n",
    "        \"\"\"\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def freeze_base_model(self):\n",
    "        \"\"\"\n",
    "        Calling this function will disable the gradient computation for the base model so that its parameters will not\n",
    "        be updated during training. Only the classification head will be updated.\n",
    "        \"\"\"\n",
    "        for param in self.wav2vec2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_values: Optional[torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_hidden_states = True if self.config.use_weighted_layer_sum else output_hidden_states\n",
    "\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # last_hidden_state # 2, 127, 768\n",
    "        # extract_features  # 2, 127, 512\n",
    "        # hidden_states  # [13] x 2, 27, 768\n",
    "\n",
    "        if self.config.use_weighted_layer_sum:\n",
    "            hidden_states = outputs[_HIDDEN_STATES_START_POSITION]\n",
    "            hidden_states = torch.stack(hidden_states, dim=1)\n",
    "            norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)\n",
    "            hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)\n",
    "        else:\n",
    "            hidden_states = outputs[0]\n",
    "\n",
    "        logits = self.classifier(hidden_states) # 2, 127, 5\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # flatten=bs*127,n_emo; flatten=bs*1,n_emo\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), torch.argmax(labels.view(-1, self.num_labels), axis=1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n",
    "            return output\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "            \"hidden_states\": outputs.hidden_states,\n",
    "            \"attentions\": outputs.attentions,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"/home2/havt/tmp/wav2vec2-base-960h\")\n",
    "config = Wav2Vec2Config.from_pretrained(\"/home2/havt/tmp/wav2vec2-base-960h\")\n",
    "config.num_labels=5\n",
    "config.use_weighted_layer_sum=True\n",
    "config.ignore_mismatched_sizes=True\n",
    "model = Wav2Vec2ForEmotionIntensityAndClassification(config)\n",
    "# model = Wav2Vec2ForEmotionIntensityAndClassification.from_pretrained(\"/home2/havt/tmp/wav2vec2-base-960h\", num_labels=5, use_weighted_layer_sum=True, ignore_mismatched_sizes=True)\n",
    "# model = Wav2Vec2ForAudioFrameClassification.from_pretrained(\"/home2/havt/tmp/wav2vec2-base-960h\", num_labels=5, use_weighted_layer_sum=True, ignore_mismatched_sizes=True)\n",
    "# model = Wav2Vec2FeatureExtractor.from_pretrained(\"/home2/havt/tmp/wav2vec2-base-960h\")\n",
    "# model = Wav2Vec2Model.from_pretrained(\"/home2/havt/tmp/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# files = os.listdir(\"/home2/havt/emotion_intensity/esd_dataset_processed\")\n",
    "# df = pd.DataFrame(files)\n",
    "# df.to_csv(\"/tmp/esd_preprocessed_files.csv\", header=[\"fname\"], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []\n",
    "# for base_dir, dirs, files in os.walk(\"/home2/havt/emotion_intensity/esd_dataset/Emotion Speech Dataset\"):\n",
    "#     for fname in files:\n",
    "#         if fname.endswith('.wav'):\n",
    "#             fpath = os.path.join(base_dir, fname)\n",
    "#             f_base_name = fname.replace(\".wav\", \"\")\n",
    "#             data.append({\"wav_path\": fpath, \"idx\": f_base_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_df = pd.DataFrame(data)\n",
    "# wav_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel_df = pd.read_csv(\"/tmp/esd_preprocessed_files.csv\")\n",
    "# mel_df['idx'] = mel_df.apply(lambda r: r['fname'].split(\".\")[0][4:15], axis=1)\n",
    "# mel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.merge(mel_df, wav_df, on='idx', how='inner')\n",
    "# df.to_csv(\"/tmp/esd_mel_wav_processed.csv\", header=[\"fname\", \"idx\", \"wav_path\"], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(\"/tmp/esd_mel_wav_processed.csv\", cache_dir=CACHE_DIR)\n",
    "ds = ds.select(range(0, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 7169.84 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 7157.67 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:01<00:00, 923.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "\n",
    "\n",
    "base_dir = \"/home2/havt/emotion_intensity/esd_dataset_processed\"\n",
    "def read_mel(row):\n",
    "    fname = row['fname']\n",
    "    mel_file = os.path.join(base_dir, fname)\n",
    "    with open(mel_file, \"rb\") as f:\n",
    "        data = pk.load(f)\n",
    "        row['mel'] = data\n",
    "    return row\n",
    "\n",
    "def extract_fname(row):\n",
    "    fname = row['fname']\n",
    "    fname = fname.split('.')[0]\n",
    "    fname = fname.split('_')\n",
    "    _, set_idx, sample_idx, emotion = fname\n",
    "    row['set_idx'] = set_idx\n",
    "    row['sample_idx'] = sample_idx\n",
    "    row['emotion'] = emotion\n",
    "    return row\n",
    "\n",
    "emotion_map = {\n",
    "    \"neutral\": 0,\n",
    "    \"happy\": 1,\n",
    "    \"sad\": 2,\n",
    "    \"angry\": 3,\n",
    "    \"surprise\": 4,\n",
    "}\n",
    "\n",
    "def emotion_to_id(row):\n",
    "    emotion = row['emotion']\n",
    "    emotion_id = emotion_map[emotion]\n",
    "    row['emotion_id'] = emotion_id\n",
    "    return row\n",
    "\n",
    "# import librosa\n",
    "from scipy.io import wavfile\n",
    "def read_wav(row):\n",
    "    audio_path = row['wav_path']\n",
    "    # wav, sr = librosa.load(audio_path)\n",
    "    _, wav = wavfile.read(audio_path)\n",
    "    row['wav'] = wav\n",
    "    return row\n",
    "\n",
    "\n",
    "# ds = ds.map(read_mel)\n",
    "ds = ds.map(extract_fname)\n",
    "ds = ds.map(emotion_to_id)\n",
    "ds = ds.map(read_wav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.with_format('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': '0003_000898',\n",
       " 'emotion_id': tensor(1),\n",
       " 'wav': tensor([32, 30, 30,  ..., -4, -7, -7])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(ds.column_names)\n",
    "ds = ds.remove_columns(['fname', 'set_idx', 'sample_idx', 'emotion', 'wav_path'])\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.remove_columns(['mel'])\n",
    "ds = ds.rename_column('wav', 'input_values')\n",
    "ds = ds.rename_column('emotion_id', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': '0003_000898',\n",
       " 'labels': tensor(1),\n",
       " 'input_values': tensor([32, 30, 30,  ..., -4, -7, -7])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "\n",
    "\n",
    "def get_mask_from_lengths(lengths, max_len=None, device=None):\n",
    "    batch_size = lengths.shape[0]\n",
    "    if max_len is None:\n",
    "        max_len = torch.max(lengths).item()\n",
    "\n",
    "    if device == \"cpu\":\n",
    "        ids = torch.arange(0, max_len).unsqueeze(0).expand(batch_size, -1)\n",
    "    elif device is not None:\n",
    "        ids = torch.arange(0, max_len).unsqueeze(0).expand(batch_size, -1).cuda()\n",
    "    elif device is None:\n",
    "        ids = torch.arange(0, max_len).unsqueeze(0).expand(batch_size, -1)\n",
    "    mask = ids >= lengths.unsqueeze(1).expand(-1, max_len)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        # input_features = [{\"input_values\": feature[\"input_values\"].type(torch.FloatTensor)} for feature in features]\n",
    "        input_features = [{\"input_values\": self.processor(audio=feature[\"input_values\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values[0]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        \n",
    "        input_feature_lens = [len(v['input_values']) for v in input_features]\n",
    "        attention_mask = get_mask_from_lengths(torch.Tensor(input_feature_lens), max_len=max(input_feature_lens))\n",
    "        \n",
    "        # print(input_features)\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"attention_mask\"] = attention_mask\n",
    "\n",
    "        return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': tensor([[ 6.9086e-04, -4.1661e-05, -1.0297e-02,  ...,  6.5510e-03,\n",
      "          6.5510e-03,  7.2835e-03],\n",
      "        [ 1.1239e-02,  1.0571e-02,  1.1239e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 'labels': tensor([4, 1]), 'attention_mask': tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ...,  True,  True,  True]])}\n"
     ]
    }
   ],
   "source": [
    "collator = DataCollatorCTCWithPadding()\n",
    "collator.processor=processor\n",
    "collator.padding=True\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collator,\n",
    ")\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.freeze_feature_extractor()\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.use_weighted_layer_sum = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_batchs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# print(batch)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# print(type(batch))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# out = model(batch.input_values, sampling_rate=16000)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# out = model(**batch)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(out)\n",
      "File \u001b[0;32m/home2/havt/miniconda3/envs/rvc/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mWav2Vec2ForEmotionIntensityAndClassification.forward\u001b[0;34m(self, input_values, attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_weighted_layer_sum:\n\u001b[1;32m     77\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[_HIDDEN_STATES_START_POSITION]\n\u001b[0;32m---> 78\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     norm_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     80\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m (hidden_states \u001b[38;5;241m*\u001b[39m norm_weights\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "n_batchs = 10\n",
    "\n",
    "for epoch in range(1, n_batchs+1):\n",
    "    for batch in loader:\n",
    "        # print(batch)\n",
    "        # print(type(batch))\n",
    "        # out = model(batch.input_values, sampling_rate=16000)\n",
    "        out = model(**batch)\n",
    "        # out = model(**batch)\n",
    "        print(out)\n",
    "        # print(torch.Tensor(out['input_values']))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([[ 2.9083561e-02,  2.5848836e-02,  2.2614107e-02, ...,\n",
       "         3.2293976e-09,  3.2293976e-09,  3.2293976e-09],\n",
       "       [-5.5489864e-02, -5.3176068e-02, -4.8548486e-02, ...,\n",
       "         3.9375644e-02,  3.0120473e-02,  3.2434266e-02]], dtype=float32)]}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch.input_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': '0003_000898',\n",
       " 'labels': tensor(1),\n",
       " 'input_values': tensor([32, 30, 30,  ..., -4, -7, -7])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#   output_dir=\"./wav2vec_out\",\n",
    "#   group_by_length=True,\n",
    "#   per_device_train_batch_size=32,\n",
    "#   evaluation_strategy=\"steps\",\n",
    "#   num_train_epochs=30,\n",
    "#   fp16=True,\n",
    "#   gradient_checkpointing=True, \n",
    "#   save_steps=500,\n",
    "#   eval_steps=500,\n",
    "#   logging_steps=500,\n",
    "#   learning_rate=1e-4,\n",
    "#   weight_decay=0.005,\n",
    "#   warmup_steps=1000,\n",
    "#   save_total_limit=2,\n",
    "# )\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_values = processor(ds[0]['labels'], return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values + 10\n",
    "ds[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "input_values = processor(ds[0][\"input_values\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values + 10\n",
    "input_values\n",
    "# # input_values.shape\n",
    "output = model(input_values)\n",
    "# # # logits = output.logits\n",
    "# # # output['input_values'][0].shape\n",
    "# # output['logits'].shape\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# # load dummy dataset and read soundfiles\n",
    "# ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\", cache_dir=CACHE_DIR)\n",
    "\n",
    "# # # tokenize\n",
    "# # input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    "\n",
    "# # # retrieve logits\n",
    "# # logits = model(input_values).logits\n",
    "\n",
    "# # # take argmax and decode\n",
    "# # predicted_ids = torch.argmax(logits, dim=-1)\n",
    "# # transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# ds = load_from_disk(\"/home2/havt/datasets/patrickvonplaten=librispeech_asr_dummy\")\n",
    "# ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'audio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home2/havt/emotion_intensity/try_wav2vec.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B2080/home2/havt/emotion_intensity/try_wav2vec.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# tokenize\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B2080/home2/havt/emotion_intensity/try_wav2vec.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m input_values \u001b[39m=\u001b[39m processor(ds[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m\"\u001b[39m, sampling_rate\u001b[39m=\u001b[39m\u001b[39m16000\u001b[39m)\u001b[39m.\u001b[39minput_values \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B2080/home2/havt/emotion_intensity/try_wav2vec.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m input_values\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mKeyError\u001b[0m: 'audio'"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values \n",
    "input_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 232, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_values)\n",
    "# logits = output.logits\n",
    "# output['input_values'][0].shape\n",
    "output['logits'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
